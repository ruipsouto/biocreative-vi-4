{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Global Variables Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODE = 'cluster' # local' for own machine or 'cluster' for cluster machine\n",
    "W2V_FILE = '~/tmp/PubMed-w2v.bin'\n",
    "TRAINSET_FILE = 'PMtask_Triage_TrainingSet.xml'\n",
    "TESTSET_FILE = 'PMtask_Triage_TestSet.xml'\n",
    "EVALSET_FILE = 'Predictions.json'\n",
    "SAVE_FIG_FILE = None\n",
    "USE_EVALUATE_SCRIPT = True\n",
    "\n",
    "# Global Variables\n",
    "MAX_SEQUENCE_LENGTH = 0\n",
    "VOCAB_SIZE = 0\n",
    "EMBEDDING_DIM = 0\n",
    "VALIDATION_SIZE = 0.1\n",
    "if MODE == 'local':\n",
    "    W2V_LIMIT = 10000\n",
    "    EPOCHS = 1\n",
    "    BATCHSIZE = 128\n",
    "    MODEL_ARC = 'dense'\n",
    "elif MODE == 'cluster':\n",
    "    W2V_LIMIT = None\n",
    "    EPOCHS = 200\n",
    "    BATCHSIZE = 128\n",
    "    MODEL_ARC = 'lstm-gpu'\n",
    "    \n",
    "if MODE != 'local' and MODE != 'cluster':\n",
    "    sys.exit('Specify a valid running mode: \\'local\\' or \\'cluster\\' ')\n",
    "if MODEL_ARC != 'dense' and MODEL_ARC != 'lstm' and MODEL_ARC != 'lstm-gpu':\n",
    "    sys.exit('Specify a valid running model: \\'dense\\' or \\'lstm\\' or \\'lstm-gpu\\' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bioc\n",
    "from bioc import biocjson\n",
    "\n",
    "def parse_dataset(filename):\n",
    "    ids = []\n",
    "    titles = []\n",
    "    abstracts = []\n",
    "    labels = []\n",
    "    \n",
    "    with bioc.BioCXMLDocumentReader(filename) as reader:\n",
    "        collection_info = reader.get_collection_info()\n",
    "        for document in reader:\n",
    "            ids.append(document.id)\n",
    "            relevant = document.infons['relevant']\n",
    "            labels.append(0 if relevant == 'no' else 1)\n",
    "            titles.append(document.passages[0].text)\n",
    "            try:\n",
    "                abstracts.append(document.passages[1].text)\n",
    "            except IndexError:\n",
    "                abstracts.append('')\n",
    "                \n",
    "        return ids, titles, abstracts, labels\n",
    "    \n",
    "def concat_text(titles, abstracts):\n",
    "    texts = []\n",
    "    \n",
    "    for i in range(0, len(titles)):\n",
    "        text = titles[i] + abstracts[i]\n",
    "        texts.append(text)\n",
    "        \n",
    "    return texts\n",
    "\n",
    "def get_max_sequence_length(texts, texts_test):\n",
    "    max_sequence_training = len(max(texts, key = len))\n",
    "    max_sequence_testing = len(max(texts_test, key = len))\n",
    "    maxlen = max_sequence_training if max_sequence_training > max_sequence_testing else max_sequence_testing\n",
    "    print(\"Max sequence length: \", maxlen)\n",
    "    \n",
    "    return maxlen\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_text(mode, texts, labels):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, MAX_SEQUENCE_LAYER)\n",
    "\n",
    "    labels = to_categorical(np.asarray(labels))\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "    \n",
    "    if(mode == 'training'):\n",
    "        return data, labels, word_index\n",
    "    else:\n",
    "        return data, labels\n",
    "    \n",
    "def training_validation_split(texts, labels, texts_shape, labels_test, validation_split):\n",
    "    indices = np.arange(texts.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data = texts[indices]\n",
    "    labels = labels[indices]\n",
    "    nb_validation_samples = int(validation_split * texts.shape[0])\n",
    "\n",
    "    X_train = texts[:-nb_validation_samples]\n",
    "    X_val = texts[-nb_validation_samples:]\n",
    "    \n",
    "    y_train = labels[:-nb_validation_samples]\n",
    "    y_val = labels[-nb_validation_samples:]\n",
    "    \n",
    "    X_test = np.asarray(texts_shape)\n",
    "    y_test = labels = to_categorical(np.asarray(labels_test))\n",
    "    \n",
    "    print('Training set size: ', X_train.shape[0])\n",
    "    print('Training targets set size: ', y_train.shape[0])\n",
    "    print('Validation set size: ', X_val.shape[0])\n",
    "    print('Validation target set size: ', y_val.shape[0])\n",
    "    print('Test set size: ', X_test.shape[0])\n",
    "    print('Test targets set size: ', y_test.shape[0])\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "def load_pretrained_w2v(file, limit_words = None):\n",
    "    embedding_space = {}\n",
    "    embedding_space = gensim.models.KeyedVectors.load_word2vec_format(file,\n",
    "                                                                      limit = limit_words, \n",
    "                                                                      binary = True)\n",
    "    print('Found %s word vectors.' % len(embedding_space.vocab))\n",
    "    VOCAB_SIZE = len(embedding_space.vocab)\n",
    "    EMBEDDING_DIM = len(embedding_space['the'])\n",
    "    return embedding_space\n",
    "\n",
    "def compute_embedding_matrix(word_index, embedding_space):\n",
    "    embedding_matrix = np.zeros((len(embedding_space.vocab) + 1, EMBEDDING_DIM))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        try: \n",
    "            embedding_vector = embedding_space[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = [0] * EMBEDDING_DIM\n",
    "\n",
    "    print('Shape of embedding matrix: ', embedding_matrix.shape)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Input\n",
    "\n",
    "def get_embedding_layer(embedding_matrix):\n",
    "    embedding_layer = Embedding(VOCAB_SIZE + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights = [embedding_matrix],\n",
    "                                input_length = MAX_SEQUENCE_LAYER,\n",
    "                                trainable = False)\n",
    "    \n",
    "    return embedding_layer\n",
    "\n",
    "def build_compile_model_Dense(embedding_layer):\n",
    "    sequence_input = Input(shape = (MAX_SEQUENCE_LENGTH,), dtype = 'int32')\n",
    "    x = embedding_layer(sequence_input)\n",
    "    flatten = layers.Flatten()(x)\n",
    "    # dense = layers.Dense(10, activation = 'relu')(flatten)\n",
    "    output = layers.Dense(1, activation = 'sigmoid')(flatten)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_compile_model_LSTM(embedding_layer):\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype = 'int32')\n",
    "    x = embedding_layer(sequence_input)\n",
    "    lstm = layers.LSTM(64)(embedding)\n",
    "    dense = layers.Dense(10, activation = 'relu')(lstm)\n",
    "    output = layers.Dense(1, activation = 'sigmoid')(dense)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_lstm_du(embedding_layer):\n",
    "    sequence_input = Input(shape = (MAX_SEQUENCE_LENGTH,), dtype = 'int32')\n",
    "    x = embedding_layer(sequence_input)\n",
    "    '''\n",
    "    Here 64 is the size(dim) of the hidden state vector as well as the output vector. Keeping return_sequence we want the output for the entire sequence. So what is the dimension of output for this layer?\n",
    "        64*70(maxlen)*2(bidirection concat)\n",
    "    CuDNNLSTM is fast implementation of LSTM layer in Keras which only runs on GPU\n",
    "    '''\n",
    "    x = layers.Bidirectional(layers.CuDNNLSTM(64, return_sequences=True))(x)\n",
    "    avg_pool = layers.GlobalAveragePooling1D()(x)\n",
    "    max_pool = layers.GlobalMaxPooling1D()(x)\n",
    "    conc = layers.concatenate([avg_pool, max_pool])\n",
    "    conc = layers.Dense(64, activation='relu')(conc)\n",
    "    conc = layers.Dropout(0.1)(conc)\n",
    "    outp = layers.Dense(1, activation=\"sigmoid\")(conc)\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def fit_model(model, X_train, y_train, X_val, y_val, epochs, batch_size, verbose = 0):\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs = epochs,\n",
    "                        verbose = verbose,\n",
    "                        validation_data = (X_val, y_val),\n",
    "                        batch_size = batch_size)\n",
    "    return history\n",
    "\n",
    "def plot_history(history, filename = None):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    if(file_name != None):\n",
    "        plt.savefig(file_filenamename)\n",
    "        \n",
    "def save_predictions(model, corpus_test, test_sequence):\n",
    "    data = {}\n",
    "    documents = []\n",
    "    # predict_count = 0\n",
    "    predictions = model.predict(test_sequence)\n",
    "    for index, row in corpus_test.iterrows():\n",
    "        doc = {}\n",
    "        infons = {}\n",
    "        doc['id'] = row['id']\n",
    "        infons['relevant'] = 'no' if predictions[index] <= 0.5 else 'yes'\n",
    "        doc['infons'] = infons\n",
    "        documents.append(doc)\n",
    "\n",
    "    data['documents'] = documents\n",
    "    with open(EVALSET_FILE, 'w') as out:\n",
    "        json.dump(data, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length:  3715\n",
      "Found 30447 unique tokens.\n",
      "Shape of data tensor: (4082, 3715)\n",
      "Shape of label tensor: (4082, 2)\n",
      "Found 18533 unique tokens.\n",
      "Shape of data tensor: (1427, 3715)\n",
      "Shape of label tensor: (1427, 2)\n",
      "Training set size:  3674\n",
      "Training targets set size:  3674\n",
      "Validation set size:  408\n",
      "Validation target set size:  408\n",
      "Test set size:  1427\n",
      "Test targets set size:  1427\n"
     ]
    }
   ],
   "source": [
    "# Load training and testing dataset\n",
    "ids, titles, abstracts, labels = parse_dataset('PMtask_Triage_TrainingSet.xml')\n",
    "texts = concat_text(titles, abstracts)\n",
    "ids_test, titles_test, abstracts_test, labels_test = parse_dataset('PMtask_Triage_TestSet.xml')\n",
    "texts_test = concat_text(titles_test, abstracts_test)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = get_max_sequence_length(texts, texts_test)\n",
    "\n",
    "# Vectorize text\n",
    "texts, labels, word_index = vectorize_text('training', texts, labels)\n",
    "texts_test, labels_test = vectorize_text('testing', texts_test, labels_test)\n",
    "\n",
    "# Training validation split\n",
    "X_train, X_val, y_train, y_val, X_test, y_test = training_validation_split(texts, labels, texts_test, labels_test, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2351706 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Loading Embedding Space\n",
    "embedding_space = load_pretrained_w2v(W2V_FILE, W2V_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix:  (2351707, 200)\n"
     ]
    }
   ],
   "source": [
    "# Compute Embedding Matrix\n",
    "embedding_matrix = compute_embedding_matrix(word_index, embedding_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Model\n",
    "# Getting the embedding layer\n",
    "embedding_layer = get_embedding_layer(embedding_matrix)\n",
    "\n",
    "if MODEL_ARC == 'dense':\n",
    "    model = build_compile_model_Dense(embedding_layer)\n",
    "elif MODEL_ARC == 'lstm':\n",
    "    model = build_compile_model_LSTM(embedding_layer)\n",
    "elif MODEL_ARC == 'lstm-gpu':\n",
    "    model = model_lstm_du(embedding_layer)\n",
    "\n",
    "\n",
    "# Model fitting and accuracy\n",
    "history = fit_model(model, X_train, y_train, X_validation, y_validation, epochs=EPOCHS, batch_size=BATCHSIZE, verbose=1)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose = False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose = False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "\n",
    "plot_history(history, SAVE_FIG_FILE)\n",
    "\n",
    "if(USE_EVALUATE_SCRIPT):\n",
    "    print('---------------------EVALUATION SCRIPT OUTPUT---------------------------')\n",
    "    save_predictions(model, corpus_test, test_word_sequence)\n",
    "    os.system('python eval_json.py triage ' + TESTSET_FILE + ' ' + EVALSET_FILE)\n",
    "    print('--------------------------END OF OUTPUT---------------------------------')        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
